import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import svm
import warnings
warnings.filterwarnings('ignore')
df = pd.read_csv('/content/new diabetes.csv') 
pd.read_csv?
df.head()
f,ax=plt.subplots(1,2,figsize=(10,5))
df['Outcome'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)
ax[0].set_title('Outcome')
ax[0].set_ylabel('')
sns.countplot('Outcome',data=df,ax=ax[1])
ax[1].set_title('Outcome')
N,P = df['Outcome'].value_counts()
print('Negative (0): ',N)
print('Positive (1): ',P)
plt.grid()
plt.show()
df.hist(bins=10,figsize=(10,10))
plt.show()
from pandas.plotting import scatter_matrix
scatter_matrix(df, figsize = (20, 20));
sns.pairplot(data = df, hue = 'Outcome' )
plt.show()
import seaborn as sns
corrmat = df.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(10,10))
g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap="RdYlGn")

from sklearn.svm import SVC
sv=SVC()
sv.fit(x_train, y_train)
from sklearn.tree import DecisionTreeClassifier
dt=DecisionTreeClassifier()
dt.fit(x_train, y_train)
from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier(criterion='entropy')
rf.fit(x_train, y_train)

from sklearn.metrics import accuracy_score
print("Train Accurancy of Logistic Regression",lr.score(x_train,y_train)*100)
print("Accurancy(Test) score of Logistic Regression",lr.score(x_train,y_train)*100)
print("Accurancy(Test) score of Logistic Regression",accuracy_score(y_test,lr_pred)*100)

print("Train Accurancy of KNN",knn.score(x_train,y_train)*100)
print("Accurancy(Test) score of KNN",knn.score(x_train,y_train)*100)
print("Accurancy(Test) score of KNN",accuracy_score(y_test,knn_pred)*100)

print("Train Accurancy of Naive Bayes",nb.score(x_train,y_train)*100)
print("Accurancy(Test) score of Naive Bayes",nb.score(x_train,y_train)*100)
print("Accurancy(Test) score of Naive Bayes",accuracy_score(y_test,lr_pred)*100)

print("Train Accurancy of Naive Bayes",nb.score(x_train,y_train)*100)
print("Accurancy(Test) score of Naive Bayes",nb.score(x_train,y_train)*100)
print("Accurancy(Test) score of Naive Bayes",accuracy_score(y_test,lr_pred)*100)

print("Train Accurancy of Decesion tree",dt.score(x_train,y_train)*100)
print("Accurancy(Test) score of Decesion tree",dt.score(x_train,y_train)*100)
print("Accurancy(Test) score of Decesion tree",accuracy_score(y_test,dt_pred)*100)


print("Train Accurancy of Random Forest",rf.score(x_train,y_train)*100)
print("Accurancy(Test) score of Random Forest",rf.score(x_train,y_train)*100)
print("Accurancy(Test) score of Random Forest",accuracy_score(y_test,rf_pred)*100)

from sklearn.metrics import classification_report,confusion_matrix
cm=confusion_matrix(y_test,lr_pred)
cm

sns.heatmap(confusion_matrix(y_test,lr_pred),annot=True,fmt="d")
import matplotlib.pyplot as plt
plt.clf()
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)
classNames = ['0','1']
plt.title('Confusion Matrix of Logistic Regression')
plt.ylabel('Actual(true) values')
plt.xlabel('Predicted values')


input_data = (5,166,72,19,175,25.8,0.587,51)

# changing the input_data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# standardize the input data
std_data = scaler.transform(input_data_reshaped)
print(std_data)

prediction = classifier.predict(std_data)
print(prediction)

if (prediction[0] == 0):
  print('The person is not diabetic')
else:
  print('The person is diabetic')
